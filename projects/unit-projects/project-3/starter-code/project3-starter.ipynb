{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "\n",
    "In this project, you will perform a logistic regression on the admissions data we've been working with in projects 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import pylab as pl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit    gre   gpa  prestige\n",
       "0      0  380.0  3.61       3.0\n",
       "1      1  660.0  3.67       3.0\n",
       "2      1  800.0  4.00       1.0\n",
       "3      1  640.0  3.19       4.0\n",
       "4      0  520.0  2.93       4.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv(\"../assets/admissions.csv\")\n",
    "df = df_raw.dropna() \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Frequency Tables\n",
    "\n",
    "#### 1. Let's create a frequency table of our variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prestige</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.459016</td>\n",
       "      <td>0.641892</td>\n",
       "      <td>0.768595</td>\n",
       "      <td>0.820896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.358108</td>\n",
       "      <td>0.231405</td>\n",
       "      <td>0.179104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "prestige       1.0       2.0       3.0       4.0\n",
       "admit                                           \n",
       "0         0.459016  0.641892  0.768595  0.820896\n",
       "1         0.540984  0.358108  0.231405  0.179104"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#frequency table for prestige and whether or not someone was admitted\n",
    "#pd.crosstab([df['admit'],df['prestige']],[df['gpa']])\n",
    "\n",
    "pd.crosstab(df['admit'],[df['prestige']]).apply(lambda x: x/sum(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: In the frequency table we are are basically counting the number of observations on the unique columns of the Dataframe.  In this case we have an admit column and a prestige column and both are categorical variables as we verified in the previous unit projects. For each admit category AND each prestige category, we want to tabulate the number of observations for each combination of categories.  I will do this using Pandas \"crosstab\" function.  There is an 'index' argument and a 'column' argument. The index argument says \"values to group-by in the rows\". The rows refer to the tabulated table rows (admit in the example above). We want admit to be the rows, so I pass in the admit column of the Dataframe like this: df['admit']. The crosstab function will know to group all the 1 observations together and the 0 observations together. Then in the column argument to crosstab,I can pass in the prestige data. Crosstab will take the groups of admit = 1 and 0 and further segment those groups by prestige levels and just do a simple count of how many obeservations are in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stat</th>\n",
       "      <th>gpa</th>\n",
       "      <th>gre</th>\n",
       "      <th>prestige</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean</td>\n",
       "      <td>3.347159</td>\n",
       "      <td>573.579336</td>\n",
       "      <td>2.645756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mean</td>\n",
       "      <td>3.489206</td>\n",
       "      <td>618.571429</td>\n",
       "      <td>2.150794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Std</td>\n",
       "      <td>0.376355</td>\n",
       "      <td>116.052798</td>\n",
       "      <td>0.918922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Std</td>\n",
       "      <td>0.371655</td>\n",
       "      <td>109.257233</td>\n",
       "      <td>0.921455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Stat       gpa         gre  prestige\n",
       "admit                                      \n",
       "0      Mean  3.347159  573.579336  2.645756\n",
       "1      Mean  3.489206  618.571429  2.150794\n",
       "0       Std  0.376355  116.052798  0.918922\n",
       "1       Std  0.371655  109.257233  0.921455"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats=df.groupby('admit').mean()\n",
    "stats['Stat']='Mean'\n",
    "stats=stats.append(df.groupby('admit').std())\n",
    "stats.fillna('Std')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Return of dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Create class or dummy variables for prestige "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prestige_1.0</th>\n",
       "      <th>prestige_2.0</th>\n",
       "      <th>prestige_3.0</th>\n",
       "      <th>prestige_4.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>397 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prestige_1.0  prestige_2.0  prestige_3.0  prestige_4.0\n",
       "0               0             0             1             0\n",
       "1               0             0             1             0\n",
       "2               1             0             0             0\n",
       "3               0             0             0             1\n",
       "4               0             0             0             1\n",
       "5               0             1             0             0\n",
       "6               1             0             0             0\n",
       "7               0             1             0             0\n",
       "8               0             0             1             0\n",
       "9               0             1             0             0\n",
       "10              0             0             0             1\n",
       "11              1             0             0             0\n",
       "12              1             0             0             0\n",
       "13              0             1             0             0\n",
       "14              1             0             0             0\n",
       "15              0             0             1             0\n",
       "16              0             0             0             1\n",
       "17              0             0             1             0\n",
       "18              0             1             0             0\n",
       "19              1             0             0             0\n",
       "20              0             0             1             0\n",
       "21              0             1             0             0\n",
       "22              0             0             0             1\n",
       "23              0             0             0             1\n",
       "24              0             1             0             0\n",
       "25              1             0             0             0\n",
       "26              1             0             0             0\n",
       "27              0             0             0             1\n",
       "28              0             1             0             0\n",
       "29              1             0             0             0\n",
       "..            ...           ...           ...           ...\n",
       "370             0             1             0             0\n",
       "371             0             0             1             0\n",
       "372             1             0             0             0\n",
       "373             1             0             0             0\n",
       "374             0             1             0             0\n",
       "375             0             0             0             1\n",
       "376             0             1             0             0\n",
       "377             0             1             0             0\n",
       "378             0             0             1             0\n",
       "379             0             1             0             0\n",
       "380             0             1             0             0\n",
       "381             0             1             0             0\n",
       "382             0             1             0             0\n",
       "383             1             0             0             0\n",
       "384             0             1             0             0\n",
       "385             1             0             0             0\n",
       "386             0             1             0             0\n",
       "387             0             1             0             0\n",
       "388             0             1             0             0\n",
       "389             0             1             0             0\n",
       "390             0             1             0             0\n",
       "391             0             1             0             0\n",
       "392             0             0             1             0\n",
       "393             0             1             0             0\n",
       "394             0             0             1             0\n",
       "395             0             1             0             0\n",
       "396             0             0             1             0\n",
       "397             0             1             0             0\n",
       "398             0             1             0             0\n",
       "399             0             0             1             0\n",
       "\n",
       "[397 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummyPrestige = pd.get_dummies(df['prestige'],prefix='prestige')\n",
    "dummyPrestige"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 When modeling our class variables, how many do we need? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Since prestige is the only categorical variable (GRE is discrete and GPA is continuous), we only need dummy variables for prestige.  Each dummy variable column is dichotomous and there are four ranks (1-4).  When creating dummy variables we set a baseline, which is typically the variable with the most occurances.  Therefore we need three additional dummy_presitige variables since we drop one of the dummy variables to avoid redundant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Hand calculating odds ratios\n",
    "\n",
    "Develop your intuition about expected outcomes by hand calculating odds ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige_1.0</th>\n",
       "      <th>prestige_2.0</th>\n",
       "      <th>prestige_3.0</th>\n",
       "      <th>prestige_4.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit    gre   gpa  prestige_1.0  prestige_2.0  prestige_3.0  prestige_4.0\n",
       "0      0  380.0  3.61             0             0             1             0\n",
       "1      1  660.0  3.67             0             0             1             0\n",
       "2      1  800.0  4.00             1             0             0             0\n",
       "3      1  640.0  3.19             0             0             0             1\n",
       "4      0  520.0  2.93             0             0             0             1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnsToKeep = ['admit', 'gre', 'gpa']\n",
    "handCalcDf = df[columnsToKeep].join(dummyPrestige.ix[:, 'prestige_1.0':])\n",
    "handCalcDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prestige_1.0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>243</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "prestige_1.0    0   1\n",
       "admit                \n",
       "0             243  28\n",
       "1              93  33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#crosstab prestige_1.0 admission \n",
    "#frequency table cutting prestige and whether or not someone was admitted\n",
    "#column 0 indicates all other schools not in prestige_1.0\n",
    "\n",
    "pd.crosstab(handCalcDf['admit'],[handCalcDf['prestige_1.0']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: When prestige_1.0 students are evaluated, 33 were admitted, and 28 were not. Amongst all other students who did not attend prestige_1.0), 93 were admitted and 243 were not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Use the cross tab above to calculate the odds of being admitted to grad school if you attended a #1 ranked college"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17857142857\n"
     ]
    }
   ],
   "source": [
    "#Prob = P(admitted in prestige_1.0)\n",
    "#prob = float(33)/61*100\n",
    "#Odds = P(something happening) / P(something not happening)\n",
    "#total students in prestige_1.0 = 33+28 = 61\n",
    "\n",
    "oddsA = float(33)/28\n",
    "print oddsA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Now calculate the odds of admission if you did not attend a #1 ranked college"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.382716049383\n"
     ]
    }
   ],
   "source": [
    "oddsB = float(93)/243\n",
    "print oddsB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Calculate the odds ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.07949308756\n"
     ]
    }
   ],
   "source": [
    "OR = oddsA/oddsB\n",
    "print OR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Write this finding in a sentenance: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The odds ratio here tells us that the odds of admission are approximately 300% higher (Odds Ratio x 100%) for students who went to a prestige_1.0 school than they are for students who did not go to a prestige_1.0 school.  I.e. for students who attended a prestige_1.0 school, the odds of being admitted into the graduate program are 3.08 times the odds of studens who did not attend a prestige_1.0 school."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Print the cross tab for prestige_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prestige_4.0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>216</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "prestige_4.0    0   1\n",
       "admit                \n",
       "0             216  55\n",
       "1             114  12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(handCalcDf['admit'],[handCalcDf['prestige_4.0']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Calculate the OR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.413397129187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5866028708133972"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#odds of admission if you attended a #4 ranked college\n",
    "oddsC = float(12)/55\n",
    "\n",
    "#odds of admission if you did not attend a #4 ranked college\n",
    "oddsD = float(114)/216\n",
    "\n",
    "OR = oddsC/oddsD\n",
    "print OR\n",
    "1-OR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Write this finding in a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The odds ratio here tells us that the odds of admission are approximately 59% lower for students who went to a prestige_4.0 school than they are for students who did not go to a prestige_4.0 school.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige_2.0</th>\n",
       "      <th>prestige_3.0</th>\n",
       "      <th>prestige_4.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit    gre   gpa  prestige_2.0  prestige_3.0  prestige_4.0\n",
       "0      0  380.0  3.61             0             1             0\n",
       "1      1  660.0  3.67             0             1             0\n",
       "2      1  800.0  4.00             0             0             0\n",
       "3      1  640.0  3.19             0             0             1\n",
       "4      0  520.0  2.93             0             0             1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a clean data frame for the regression\n",
    "#we only keep prestige_2, prestige_3, and prestige_4 because we do not want redundant information in our variables\n",
    "\n",
    "columnsToKeep = ['admit', 'gre', 'gpa']\n",
    "data = df[columnsToKeep].join(dummyPrestige.ix[:, 'prestige_2.0':])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to add a constant term for our Logistic Regression. The statsmodels function we're going to be using requires that intercepts/constants are specified explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manually add the intercept if using statsmodels\n",
    "data['intercept'] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:  Adding an intercept column of all 1.0's actually represents an intercept term in the regression because if one of the predictors is always 1.0, its Beta value will be one (i.e. no change on overall results), and so it effectively just adds 1.0 to the regression = the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Set the covariates to a variable called train_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     gre   gpa  prestige_2.0  prestige_3.0  prestige_4.0  intercept\n",
      "0  380.0  3.61             0             1             0        1.0\n",
      "1  660.0  3.67             0             1             0        1.0\n",
      "2  800.0  4.00             0             0             0        1.0\n",
      "3  640.0  3.19             0             0             1        1.0\n",
      "4  520.0  2.93             0             0             1        1.0\n",
      "     gre   gpa  prestige_2.0  prestige_3.0  prestige_4.0\n",
      "0  380.0  3.61             0             1             0\n",
      "1  660.0  3.67             0             1             0\n",
      "2  800.0  4.00             0             0             0\n",
      "3  640.0  3.19             0             0             1\n",
      "4  520.0  2.93             0             0             1\n"
     ]
    }
   ],
   "source": [
    "#These are essentially our X values\n",
    "#train_cols = ['gre','gpa','prestige_2.0','prestige_3.0','prestige_4.0','intercept']\n",
    "\n",
    "train_cols_stats = data.ix[:,1:]\n",
    "print train_cols_stats.head()\n",
    "\n",
    "train_cols = data.ix[:,1:6]\n",
    "print train_cols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.573854\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "#logistic regression using statsmodels\n",
    "#MLE = maximum likelihood estimate\n",
    "\n",
    "logModel = sm.Logit(data['admit'],train_cols_stats)\n",
    "resultFitStat = logModel.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Model fitting is a procedure that can be summarized in three steps...\n",
    "(1) First you need a function that takes in a set of parameters and returns a predicted data set.\n",
    "(2) Second you need an 'error function' that provides a number representing the difference between your data and the model's prediction for any given set of model parameters. This is usually either the sums of squared error (SSE) or maximum likelihood.\n",
    "(3) Third you need to find the parameters that minimize this difference.\n",
    "The fit process here involves two steps...\n",
    "(1) Create a Logit object passing in the correct arguments. \"endog\" is the dependent variable according to the documentation and \"exog\" is the independent variables. \n",
    "(2) Fit the model as described in the documentation: http://statsmodels.sourceforge.net/0.6.0/generated/statsmodels.discrete.discrete_model.Logit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  admit   No. Observations:                  397\n",
      "Model:                          Logit   Df Residuals:                      391\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                Wed, 01 Feb 2017   Pseudo R-squ.:                 0.08166\n",
      "Time:                        15:29:03   Log-Likelihood:                -227.82\n",
      "converged:                       True   LL-Null:                       -248.08\n",
      "                                        LLR p-value:                 1.176e-07\n",
      "================================================================================\n",
      "                   coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "--------------------------------------------------------------------------------\n",
      "gre              0.0022      0.001      2.028      0.043      7.44e-05     0.004\n",
      "gpa              0.7793      0.333      2.344      0.019         0.128     1.431\n",
      "prestige_2.0    -0.6801      0.317     -2.146      0.032        -1.301    -0.059\n",
      "prestige_3.0    -1.3387      0.345     -3.882      0.000        -2.015    -0.663\n",
      "prestige_4.0    -1.5534      0.417     -3.721      0.000        -2.372    -0.735\n",
      "intercept       -3.8769      1.142     -3.393      0.001        -6.116    -1.638\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#statsmodels results\n",
    "\n",
    "summary = resultFitStat.summary()\n",
    "print summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#logistic regression using sklearn\n",
    "\n",
    "import sklearn.linear_model as lm\n",
    "\n",
    "logModel = lm.LogisticRegression()\n",
    "resultFit = logModel.fit(train_cols,data['admit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00178497  0.23229458 -0.60347467 -1.17214957 -1.37729795]]\n",
      "[-1.81701706]\n",
      "0.317380352645\n"
     ]
    }
   ],
   "source": [
    "#sklearn results\n",
    "\n",
    "print resultFit.coef_\n",
    "print resultFit.intercept_\n",
    "print df.admit.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Print the summary results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.705289672544\n",
      "[ 0.18659419  0.27716576  0.63184466  0.21233404  0.17001703  0.40925899\n",
      "  0.46874205  0.27070755  0.22481859  0.43526125  0.3021334   0.42956137\n",
      "  0.61508543  0.38804338  0.58943772  0.2086162   0.28842267  0.14780737\n",
      "  0.4696848   0.50797682  0.20419042  0.40150911  0.18720704  0.2245205\n",
      "  0.42905043  0.61328896  0.53201833  0.19823728  0.43039855  0.46889457\n",
      "  0.20548533  0.2985104   0.24445981  0.3470547   0.39054492  0.26933392\n",
      "  0.49330638  0.19983482  0.30982793  0.1917883   0.29759959  0.35114953\n",
      "  0.35028634  0.20952587  0.38034939  0.20315947  0.35859467  0.16632301\n",
      "  0.13790066  0.18287472  0.27885915  0.15685126  0.2515047   0.39004865\n",
      "  0.26207308  0.32319895  0.22294903  0.16411395  0.29762611  0.18720704\n",
      "  0.36005798  0.19411147  0.27007075  0.29295988  0.26411199  0.37388842\n",
      "  0.26259399  0.51405567  0.51884944  0.61713824  0.28544585  0.12126306\n",
      "  0.17507905  0.38792808  0.24829637  0.31543981  0.22986517  0.3470547\n",
      "  0.46795069  0.55449361  0.21905523  0.35419158  0.2893614   0.13704555\n",
      "  0.22090155  0.31001084  0.35932599  0.36792625  0.54844488  0.42232602\n",
      "  0.43012938  0.57779175  0.47837212  0.33079645  0.39095086  0.38488386\n",
      "  0.22543736  0.32422693  0.37707003  0.18149032  0.16103486  0.24516062\n",
      "  0.14899759  0.24786319  0.41949499  0.39898459  0.56449662  0.30224643\n",
      "  0.16874403  0.3181495   0.22010288  0.16077103  0.16115043  0.50048862\n",
      "  0.30746962  0.27775423  0.30286585  0.42387752  0.61549032  0.15394668\n",
      "  0.34898023  0.28019818  0.19798409  0.19711203  0.30945167  0.19072987\n",
      "  0.51906295  0.26802733  0.32836543  0.16226105  0.35952291  0.34750873\n",
      "  0.3553953   0.20081418  0.3239586   0.21970452  0.19447511  0.30778223\n",
      "  0.37191557  0.52138209  0.40970327  0.24468598  0.23642291  0.23792645\n",
      "  0.2027443   0.18033094  0.31563525  0.20422987  0.4294113   0.56776963\n",
      "  0.61768695  0.28468344  0.41026519  0.24046282  0.26399297  0.18322198\n",
      "  0.30247805  0.46789024  0.3937199   0.29894617  0.34922974  0.38578168\n",
      "  0.41482776  0.29772763  0.35009173  0.58943772  0.16026024  0.30401809\n",
      "  0.23730672  0.25402162  0.18849425  0.20221073  0.27547872  0.45121966\n",
      "  0.18515127  0.37028915  0.29241274  0.24376036  0.24806796  0.14748448\n",
      "  0.26807511  0.19773263  0.43983463  0.36248474  0.16060583  0.46101581\n",
      "  0.24030665  0.33009327  0.32086073  0.35545092  0.32325207  0.27421186\n",
      "  0.15681915  0.3673862   0.35733037  0.2501259   0.15058568  0.25446206\n",
      "  0.21699227  0.31167627  0.34482496  0.58943772  0.1774045   0.53931663\n",
      "  0.32867823  0.58083079  0.54204898  0.21564452  0.36073462  0.25019509\n",
      "  0.33544079  0.25699826  0.37225848  0.24321965  0.36900857  0.46247623\n",
      "  0.42465513  0.35733037  0.22039677  0.29974121  0.43568305  0.31970516\n",
      "  0.42095357  0.29091258  0.38173401  0.17815459  0.31714252  0.41559946\n",
      "  0.19864539  0.239348    0.27746986  0.16530806  0.60610292  0.3531296\n",
      "  0.34648476  0.16281007  0.2812499   0.27012841  0.49905264  0.25203392\n",
      "  0.36220173  0.46332677  0.34233239  0.39392407  0.28931148  0.28295847\n",
      "  0.27282711  0.22316117  0.20718678  0.36282398  0.1968998   0.25812074\n",
      "  0.25566981  0.32613149  0.27589409  0.33694182  0.40297908  0.38124319\n",
      "  0.28835717  0.21082468  0.27589409  0.23957108  0.17726134  0.19122092\n",
      "  0.254903    0.40059856  0.19093704  0.41082734  0.22239891  0.41236676\n",
      "  0.53302242  0.3170099   0.34135294  0.21545885  0.51246769  0.21692947\n",
      "  0.37448924  0.41892942  0.16981012  0.22884978  0.17566235  0.29993119\n",
      "  0.20514593  0.58878595  0.23925947  0.26219139  0.12790159  0.37463272\n",
      "  0.41417429  0.41405647  0.63022211  0.40905352  0.21315294  0.47916656\n",
      "  0.29188215  0.34443863  0.28614413  0.37483311  0.27391986  0.27392974\n",
      "  0.42991975  0.12574905  0.20500651  0.47084348  0.36127047  0.32529897\n",
      "  0.18071049  0.25723223  0.40374395  0.28184366  0.19493537  0.19361478\n",
      "  0.22700797  0.24662064  0.27711696  0.18607541  0.47721426  0.19175067\n",
      "  0.31695733  0.16761541  0.26000719  0.19250947  0.57510549  0.39226151\n",
      "  0.35148401  0.3522088   0.16503861  0.32319895  0.27345809  0.40619033\n",
      "  0.23275638  0.21039811  0.53779732  0.20928288  0.20264533  0.47258011\n",
      "  0.26795496  0.17486718  0.20197406  0.2003248   0.33751527  0.21707491\n",
      "  0.19895663  0.36093186  0.37695598  0.2837383   0.34259084  0.47526134\n",
      "  0.25242626  0.22633479  0.41257402  0.3592701   0.54792967  0.39857449\n",
      "  0.55898842  0.24371561  0.31243791  0.51008527  0.48939552  0.38288847\n",
      "  0.30995891  0.49077027  0.1844324   0.18430379  0.2612938   0.53679457\n",
      "  0.4777925   0.35873555  0.28864824  0.48973059  0.5181166   0.36752855\n",
      "  0.2003637   0.38447898  0.48417127  0.24563574  0.30376949  0.41991159\n",
      "  0.3439143   0.34496334  0.57205552  0.27786163  0.40955644  0.44943455\n",
      "  0.35326952  0.36778383  0.38633226  0.4294695   0.41554049  0.24360274\n",
      "  0.3910967   0.22422295  0.40501045  0.21697084  0.27121438  0.41991159\n",
      "  0.26608885]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#calculating the accuracy with sklearn\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = train_cols\n",
    "y = df['admit']\n",
    "\n",
    "predicted = resultFit.predict(X)\n",
    "threshold = 0.5\n",
    "predictedClasses = (predicted > threshold).astype(int)\n",
    "print accuracy_score(y, predictedClasses)\n",
    "\n",
    "predictedProba1 = resultFit.predict_proba(X)\n",
    "predictedProba2 = predictedProba1[:,1]\n",
    "print predictedProba2\n",
    "\n",
    "print predictedClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.7037037   0.73417722  0.70886076  0.67088608  0.69620253]\n"
     ]
    }
   ],
   "source": [
    "#k-folds cross validation with sklearn\n",
    "\n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "\n",
    "scores = cross_val_score(resultFit,train_cols, data.admit, cv=5)\n",
    "print scores  \n",
    "\n",
    "#kf = KFold(4, n_folds=2)\n",
    "#mylist = list(kf)\n",
    "#train, test = mylist[0]\n",
    "#for train_index, test_index in kf:\n",
    " #   print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "See above. The confidence interval is the lower and upper range for the distribution of coefficients for each variable.  The confidence interval contains the parameter values that, when tested, should not be rejected with the same sample.  Confidence intervals of difference parameters not containing 0 imply that there is a statistically significant difference between the populations.  Therefore, the significant predictors are the prestige categories and the GPA (somewhat of a strong predictor), but GRE is not.  The coefficients represent the Beta values for the fit function.  From the summary printed above, we can see that there is an inverse relationship between the probability of admission and the prestige of the school.  Thus, the probability of admission is higher for students who attended a top ranked prestigue_1.0 school.  \n",
    "\n",
    "Sklearn tells us that he accuracy of the model with all features (removing one rank) is about 70%.  I then performed k-folds and found that the accuracy is roughly 70% across all five folds.  The goal is to get your accuracy score consistent accross all of your folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.565439582967\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAFyCAYAAAB7mplaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJguSJKmq/w9HuMvF6q2auwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116f93e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import dummy, metrics\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "vals = metrics.roc_curve(df.admit,predictedProba2)\n",
    "ax.plot(vals[0], vals[1])\n",
    "ax.set(title='Area Under the Curve', ylabel='', xlabel='', xlim=(0, 1), ylim=(0, 1))\n",
    "\n",
    "AUC = metrics.roc_auc_score(df.admit, predictedClasses)\n",
    "print AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Calculate the odds ratios of the coeffiencents and their 95% CI intervals\n",
    "\n",
    "hint 1: np.exp(X)\n",
    "\n",
    "hint 2: conf['OR'] = params\n",
    "        \n",
    "           conf.columns = ['2.5%', '97.5%', 'OR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gre</th>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.004362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpa</th>\n",
       "      <td>0.127619</td>\n",
       "      <td>1.431056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prestige_2.0</th>\n",
       "      <td>-1.301337</td>\n",
       "      <td>-0.058936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prestige_3.0</th>\n",
       "      <td>-2.014579</td>\n",
       "      <td>-0.662776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prestige_4.0</th>\n",
       "      <td>-2.371624</td>\n",
       "      <td>-0.735197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>-6.116077</td>\n",
       "      <td>-1.637631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0         1\n",
       "gre           0.000074  0.004362\n",
       "gpa           0.127619  1.431056\n",
       "prestige_2.0 -1.301337 -0.058936\n",
       "prestige_3.0 -2.014579 -0.662776\n",
       "prestige_4.0 -2.371624 -0.735197\n",
       "intercept    -6.116077 -1.637631"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using Statsmodels\n",
    "resultFitStat.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gre             1.002221\n",
       "gpa             2.180027\n",
       "prestige_2.0    0.506548\n",
       "prestige_3.0    0.262192\n",
       "prestige_4.0    0.211525\n",
       "intercept       0.020716\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can easily convert these into odds using numpy.exp()\n",
    "\n",
    "np.exp(resultFitStat.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2.5%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>OR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gre</th>\n",
       "      <td>1.000074</td>\n",
       "      <td>1.004372</td>\n",
       "      <td>1.002221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpa</th>\n",
       "      <td>1.136120</td>\n",
       "      <td>4.183113</td>\n",
       "      <td>2.180027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prestige_2.0</th>\n",
       "      <td>0.272168</td>\n",
       "      <td>0.942767</td>\n",
       "      <td>0.506548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prestige_3.0</th>\n",
       "      <td>0.133377</td>\n",
       "      <td>0.515419</td>\n",
       "      <td>0.262192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prestige_4.0</th>\n",
       "      <td>0.093329</td>\n",
       "      <td>0.479411</td>\n",
       "      <td>0.211525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>0.002207</td>\n",
       "      <td>0.194440</td>\n",
       "      <td>0.020716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  2.5%     97.5%        OR\n",
       "gre           1.000074  1.004372  1.002221\n",
       "gpa           1.136120  4.183113  2.180027\n",
       "prestige_2.0  0.272168  0.942767  0.506548\n",
       "prestige_3.0  0.133377  0.515419  0.262192\n",
       "prestige_4.0  0.093329  0.479411  0.211525\n",
       "intercept     0.002207  0.194440  0.020716"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = resultFitStat.params\n",
    "conf = resultFitStat.conf_int()\n",
    "conf['OR'] = params\n",
    "conf.columns = ['2.5%', '97.5%', 'OR']\n",
    "np.exp(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Interpret the OR of Prestige_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    397.000000\n",
       "mean       0.372796\n",
       "std        0.484159\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        1.000000\n",
       "max        1.000000\n",
       "Name: prestige_2.0, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['prestige_2.0'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: From the OR generated above, we can see that the odds of admission decreases 50% if a student goes to a prestige_2.0 school (as compared to the baseline prestige_1.0 school)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 Interpret the OR of GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    397.000000\n",
       "mean       3.392242\n",
       "std        0.380208\n",
       "min        2.260000\n",
       "25%        3.130000\n",
       "50%        3.400000\n",
       "75%        3.670000\n",
       "max        4.000000\n",
       "Name: gpa, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['gpa'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The odds ratio here tells us that the odds of admission are approximately 218% higher (Odds Ratio x 100%) for every one-unit increase in GPA (e.g. from 3.0 to 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Predicted probablities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a way of evaluating our classifier, we're going to recreate the dataset with every logical combination of input values. This will allow us to see how the predicted probability of admission increases/decreases across different variables. First we're going to generate the combinations using a helper function called cartesian (above).\n",
    "\n",
    "We're going to use np.linspace to create a range of values for \"gre\" and \"gpa\". This creates a range of linearly spaced values from a specified min and maximum value--in our case just the min/max observed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cartesian(arrays, out=None):\n",
    "    \"\"\"\n",
    "    Generate a cartesian product of input arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    arrays : list of array-like\n",
    "        1-D arrays to form the cartesian product of.\n",
    "    out : ndarray\n",
    "        Array to place the cartesian product in.\n",
    "    Returns\n",
    "    -------\n",
    "    out : ndarray\n",
    "        2-D array of shape (M, len(arrays)) containing cartesian products\n",
    "        formed of input arrays.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> cartesian(([1, 2, 3], [4, 5], [6, 7]))\n",
    "    array([[1, 4, 6],\n",
    "           [1, 4, 7],\n",
    "           [1, 5, 6],\n",
    "           [1, 5, 7],\n",
    "           [2, 4, 6],\n",
    "           [2, 4, 7],\n",
    "           [2, 5, 6],\n",
    "           [2, 5, 7],\n",
    "           [3, 4, 6],\n",
    "           [3, 4, 7],\n",
    "           [3, 5, 6],\n",
    "           [3, 5, 7]])\n",
    "    \"\"\"\n",
    "\n",
    "    arrays = [np.asarray(x) for x in arrays]\n",
    "    dtype = arrays[0].dtype\n",
    "\n",
    "    n = np.prod([x.size for x in arrays])\n",
    "    if out is None:\n",
    "        out = np.zeros([n, len(arrays)], dtype=dtype)\n",
    "\n",
    "    m = n / arrays[0].size\n",
    "    out[:,0] = np.repeat(arrays[0], m)\n",
    "    if arrays[1:]:\n",
    "        cartesian(arrays[1:], out=out[0:m,1:])\n",
    "        for j in xrange(1, arrays[0].size):\n",
    "            out[j*m:(j+1)*m,1:] = out[0:m,1:]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 220.          284.44444444  348.88888889  413.33333333  477.77777778\n",
      "  542.22222222  606.66666667  671.11111111  735.55555556  800.        ]\n",
      "[ 2.26        2.45333333  2.64666667  2.84        3.03333333  3.22666667\n",
      "  3.42        3.61333333  3.80666667  4.        ]\n"
     ]
    }
   ],
   "source": [
    "# instead of generating all possible values of GRE and GPA, we're going\n",
    "# to use an evenly spaced range of 10 values from the min to the max \n",
    "gres = np.linspace(data['gre'].min(), data['gre'].max(), 10)\n",
    "print gres\n",
    "# array([ 220.        ,  284.44444444,  348.88888889,  413.33333333,\n",
    "#         477.77777778,  542.22222222,  606.66666667,  671.11111111,\n",
    "#         735.55555556,  800.        ])\n",
    "gpas = np.linspace(data['gpa'].min(), data['gpa'].max(), 10)\n",
    "print gpas\n",
    "# array([ 2.26      ,  2.45333333,  2.64666667,  2.84      ,  3.03333333,\n",
    "#         3.22666667,  3.42      ,  3.61333333,  3.80666667,  4.        ])\n",
    "\n",
    "\n",
    "# enumerate all possibilities\n",
    "combos = pd.DataFrame(cartesian([gres, gpas, [1, 2, 3, 4], [1.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Recreate the dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#here we won't know what actually happened because we do not have an 'admit' column\n",
    "#so we cannot make a confusion matrix\n",
    "#the purpose here is how to predict against my original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#recreate the dummy variables\n",
    "combos.columns = ['gre', 'gpa', 'prestige', 'intercept']\n",
    "dummy_prestige2 = pd.get_dummies(combos['prestige'],prefix = 'prestige')\n",
    "dummy_prestige2.columns = ['prestige_1.0', 'prestige_2.0', 'prestige_3.0', 'prestige_4.0']\n",
    "\n",
    "#keep only what we need for making predictions\n",
    "columnsToKeep2 = ['gre', 'gpa', 'prestige', 'intercept']\n",
    "combos = combos[columnsToKeep2].join(dummy_prestige2.ix[:, 'prestige_2.0':])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 7)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combos.head()\n",
    "combos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Make predictions on the enumerated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige_2.0</th>\n",
       "      <th>prestige_3.0</th>\n",
       "      <th>prestige_4.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>220.0</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>220.0</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220.0</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>220.0</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>220.0</td>\n",
       "      <td>2.453333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     gre       gpa  prestige_2.0  prestige_3.0  prestige_4.0\n",
       "0  220.0  2.260000             0             0             0\n",
       "1  220.0  2.260000             1             0             0\n",
       "2  220.0  2.260000             0             1             0\n",
       "3  220.0  2.260000             0             0             1\n",
       "4  220.0  2.453333             0             0             0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cols_new = combos[['gre', 'gpa', 'prestige_2.0', 'prestige_3.0','prestige_4.0']]\n",
    "train_cols_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_cols_new)\n",
    "train_cols_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gre             400\n",
       "gpa             400\n",
       "prestige_2.0    400\n",
       "prestige_3.0    400\n",
       "prestige_4.0    400\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cols_new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.71081335  0.28918665]\n",
      " [ 0.81799353  0.18200647]\n",
      " [ 0.88810083  0.11189917]\n",
      " [ 0.90692343  0.09307657]\n",
      " [ 0.70149503  0.29850497]\n",
      " [ 0.81121157  0.18878843]\n",
      " [ 0.88355936  0.11644064]\n",
      " [ 0.90306249  0.09693751]\n",
      " [ 0.69200656  0.30799344]\n",
      " [ 0.80423738  0.19576262]\n",
      " [ 0.8788587   0.1211413 ]\n",
      " [ 0.89905922  0.10094078]\n",
      " [ 0.68235306  0.31764694]\n",
      " [ 0.79706999  0.20293001]\n",
      " [ 0.87399535  0.12600465]\n",
      " [ 0.89490987  0.10509013]\n",
      " [ 0.67254016  0.32745984]\n",
      " [ 0.78970881  0.21029119]\n",
      " [ 0.86896586  0.13103414]\n",
      " [ 0.8906107   0.1093893 ]\n",
      " [ 0.66257402  0.33742598]\n",
      " [ 0.78215358  0.21784642]\n",
      " [ 0.86376691  0.13623309]\n",
      " [ 0.88615803  0.11384197]\n",
      " [ 0.65246131  0.34753869]\n",
      " [ 0.77440445  0.22559555]\n",
      " [ 0.85839531  0.14160469]\n",
      " [ 0.88154822  0.11845178]\n",
      " [ 0.64220917  0.35779083]\n",
      " [ 0.76646198  0.23353802]\n",
      " [ 0.85284798  0.14715202]\n",
      " [ 0.8767777   0.1232223 ]\n",
      " [ 0.63182527  0.36817473]\n",
      " [ 0.75832715  0.24167285]\n",
      " [ 0.84712204  0.15287796]\n",
      " [ 0.87184299  0.12815701]\n",
      " [ 0.6213177   0.3786823 ]\n",
      " [ 0.75000138  0.24999862]\n",
      " [ 0.84121478  0.15878522]\n",
      " [ 0.86674069  0.13325931]\n",
      " [ 0.68660742  0.31339258]\n",
      " [ 0.80023771  0.19976229]\n",
      " [ 0.87614885  0.12385115]\n",
      " [ 0.89674816  0.10325184]\n",
      " [ 0.67686371  0.32313629]\n",
      " [ 0.79296158  0.20703842]\n",
      " [ 0.87119266  0.12880734]\n",
      " [ 0.89251516  0.10748484]\n",
      " [ 0.666964    0.333036  ]\n",
      " [ 0.78549148  0.21450852]\n",
      " [ 0.86606844  0.13393156]\n",
      " [ 0.88813027  0.11186973]\n",
      " [ 0.65691472  0.34308528]\n",
      " [ 0.77782736  0.22217264]\n",
      " [ 0.86077295  0.13922705]\n",
      " [ 0.88358984  0.11641016]\n",
      " [ 0.64672283  0.35327717]\n",
      " [ 0.7699696   0.2300304 ]\n",
      " [ 0.85530307  0.14469693]\n",
      " [ 0.87889025  0.12110975]\n",
      " [ 0.63639576  0.36360424]\n",
      " [ 0.761919    0.238081  ]\n",
      " [ 0.84965582  0.15034418]\n",
      " [ 0.87402798  0.12597202]\n",
      " [ 0.62594141  0.37405859]\n",
      " [ 0.75367677  0.24632323]\n",
      " [ 0.84382842  0.15617158]\n",
      " [ 0.8689996   0.1310004 ]\n",
      " [ 0.61536814  0.38463186]\n",
      " [ 0.74524462  0.25475538]\n",
      " [ 0.83781825  0.16218175]\n",
      " [ 0.86380178  0.13619822]\n",
      " [ 0.60468476  0.39531524]\n",
      " [ 0.73662468  0.26337532]\n",
      " [ 0.83162295  0.16837705]\n",
      " [ 0.85843132  0.14156868]\n",
      " [ 0.59390047  0.40609953]\n",
      " [ 0.7278196   0.2721804 ]\n",
      " [ 0.82524034  0.17475966]\n",
      " [ 0.85288516  0.14711484]\n",
      " [ 0.66134069  0.33865931]\n",
      " [ 0.781213    0.218787  ]\n",
      " [ 0.86311704  0.13688296]\n",
      " [ 0.88560083  0.11439917]\n",
      " [ 0.65121047  0.34878953]\n",
      " [ 0.77344011  0.22655989]\n",
      " [ 0.85772403  0.14227597]\n",
      " [ 0.88097149  0.11902851]\n",
      " [ 0.64094175  0.35905825]\n",
      " [ 0.76547397  0.23452603]\n",
      " [ 0.85215494  0.14784506]\n",
      " [ 0.876181    0.123819  ]\n",
      " [ 0.63054222  0.36945778]\n",
      " [ 0.75731562  0.24268438]\n",
      " [ 0.84640689  0.15359311]\n",
      " [ 0.8712259   0.1287741 ]\n",
      " [ 0.62002006  0.37997994]\n",
      " [ 0.74896654  0.25103346]\n",
      " [ 0.8404772   0.1595228 ]\n",
      " [ 0.86610281  0.13389719]\n",
      " [ 0.60938387  0.39061613]\n",
      " [ 0.74042867  0.25957133]\n",
      " [ 0.83436338  0.16563662]\n",
      " [ 0.86080846  0.13919154]\n",
      " [ 0.59864268  0.40135732]\n",
      " [ 0.73170445  0.26829555]\n",
      " [ 0.82806318  0.17193682]\n",
      " [ 0.85533974  0.14466026]\n",
      " [ 0.58780592  0.41219408]\n",
      " [ 0.72279678  0.27720322]\n",
      " [ 0.82157459  0.17842541]\n",
      " [ 0.84969367  0.15030633]\n",
      " [ 0.57688337  0.42311663]\n",
      " [ 0.71370908  0.28629092]\n",
      " [ 0.81489587  0.18510413]\n",
      " [ 0.84386746  0.15613254]\n",
      " [ 0.56588513  0.43411487]\n",
      " [ 0.70444527  0.29555473]\n",
      " [ 0.80802557  0.19197443]\n",
      " [ 0.83785851  0.16214149]\n",
      " [ 0.63511944  0.36488056]\n",
      " [ 0.76091776  0.23908224]\n",
      " [ 0.84895041  0.15104959]\n",
      " [ 0.87341988  0.12658012]\n",
      " [ 0.62465004  0.37534996]\n",
      " [ 0.75265213  0.24734787]\n",
      " [ 0.84310071  0.15689929]\n",
      " [ 0.86837088  0.13162912]\n",
      " [ 0.61406278  0.38593722]\n",
      " [ 0.7441968   0.2558032 ]\n",
      " [ 0.83706795  0.16293205]\n",
      " [ 0.86315205  0.13684795]\n",
      " [ 0.60336651  0.39663349]\n",
      " [ 0.73555399  0.26444601]\n",
      " [ 0.83084977  0.16915023]\n",
      " [ 0.85776019  0.14223981]\n",
      " [ 0.59257048  0.40742952]\n",
      " [ 0.72672639  0.27327361]\n",
      " [ 0.82444404  0.17555596]\n",
      " [ 0.85219226  0.14780774]\n",
      " [ 0.58168433  0.41831567]\n",
      " [ 0.71771721  0.28228279]\n",
      " [ 0.81784892  0.18215108]\n",
      " [ 0.8464454   0.1535546 ]\n",
      " [ 0.57071803  0.42928197]\n",
      " [ 0.70853014  0.29146986]\n",
      " [ 0.81106281  0.18893719]\n",
      " [ 0.84051692  0.15948308]\n",
      " [ 0.55968186  0.44031814]\n",
      " [ 0.69916939  0.30083061]\n",
      " [ 0.80408445  0.19591555]\n",
      " [ 0.83440432  0.16559568]\n",
      " [ 0.54858638  0.45141362]\n",
      " [ 0.6896397   0.3103603 ]\n",
      " [ 0.79691288  0.20308712]\n",
      " [ 0.82810536  0.17189464]\n",
      " [ 0.53744239  0.46255761]\n",
      " [ 0.67994632  0.32005368]\n",
      " [ 0.7895475   0.2104525 ]\n",
      " [ 0.82161802  0.17838198]\n",
      " [ 0.60807113  0.39192887]\n",
      " [ 0.73936797  0.26063203]\n",
      " [ 0.83360027  0.16639973]\n",
      " [ 0.86014676  0.13985324]\n",
      " [ 0.5973177   0.4026823 ]\n",
      " [ 0.73062107  0.26937893]\n",
      " [ 0.82727705  0.17272295]\n",
      " [ 0.85465643  0.14534357]\n",
      " [ 0.58646987  0.41353013]\n",
      " [ 0.72169111  0.27830889]\n",
      " [ 0.82076522  0.17923478]\n",
      " [ 0.8489884   0.1510116 ]\n",
      " [ 0.57553748  0.42446252]\n",
      " [ 0.71258158  0.28741842]\n",
      " [ 0.81406306  0.18593694]\n",
      " [ 0.8431399   0.1568601 ]\n",
      " [ 0.56453067  0.43546933]\n",
      " [ 0.70329646  0.29670354]\n",
      " [ 0.80716916  0.19283084]\n",
      " [ 0.83710836  0.16289164]\n",
      " [ 0.5534599   0.4465401 ]\n",
      " [ 0.69384026  0.30615974]\n",
      " [ 0.80008243  0.19991757]\n",
      " [ 0.8308914   0.1691086 ]\n",
      " [ 0.54233584  0.45766416]\n",
      " [ 0.684218    0.315782  ]\n",
      " [ 0.79280212  0.20719788]\n",
      " [ 0.82448692  0.17551308]\n",
      " [ 0.53116941  0.46883059]\n",
      " [ 0.67443522  0.32556478]\n",
      " [ 0.78532781  0.21467219]\n",
      " [ 0.81789305  0.18210695]\n",
      " [ 0.51997167  0.48002833]\n",
      " [ 0.66449799  0.33550201]\n",
      " [ 0.7776595   0.2223405 ]\n",
      " [ 0.81110821  0.18889179]\n",
      " [ 0.50875384  0.49124616]\n",
      " [ 0.65441286  0.34558714]\n",
      " [ 0.76979756  0.23020244]\n",
      " [ 0.80413112  0.19586888]\n",
      " [ 0.58034261  0.41965739]\n",
      " [ 0.71659923  0.28340077]\n",
      " [ 0.81702641  0.18297359]\n",
      " [ 0.84572766  0.15427234]\n",
      " [ 0.56936717  0.43063283]\n",
      " [ 0.7073906   0.2926094 ]\n",
      " [ 0.81021677  0.18978323]\n",
      " [ 0.83977671  0.16022329]\n",
      " [ 0.55832315  0.44167685]\n",
      " [ 0.69800886  0.30199114]\n",
      " [ 0.80321474  0.19678526]\n",
      " [ 0.83364136  0.16635864]\n",
      " [ 0.54722113  0.45277887]\n",
      " [ 0.68845878  0.31154122]\n",
      " [ 0.79601941  0.20398059]\n",
      " [ 0.82731938  0.17268062]\n",
      " [ 0.53607194  0.46392806]\n",
      " [ 0.6787457   0.3212543 ]\n",
      " [ 0.78863022  0.21136978]\n",
      " [ 0.8208088   0.1791912 ]\n",
      " [ 0.52488656  0.47511344]\n",
      " [ 0.66887544  0.33112456]\n",
      " [ 0.78104698  0.21895302]\n",
      " [ 0.8141079   0.1858921 ]\n",
      " [ 0.51367616  0.48632384]\n",
      " [ 0.65885434  0.34114566]\n",
      " [ 0.7732699   0.2267301 ]\n",
      " [ 0.80721527  0.19278473]\n",
      " [ 0.50245198  0.49754802]\n",
      " [ 0.64868926  0.35131074]\n",
      " [ 0.7652996   0.2347004 ]\n",
      " [ 0.80012982  0.19987018]\n",
      " [ 0.49122533  0.50877467]\n",
      " [ 0.63838753  0.36161247]\n",
      " [ 0.7571371   0.2428629 ]\n",
      " [ 0.79285078  0.20714922]\n",
      " [ 0.48000752  0.51999248]\n",
      " [ 0.62795696  0.37204304]\n",
      " [ 0.74878392  0.25121608]\n",
      " [ 0.78537776  0.21462224]\n",
      " [ 0.55209735  0.44790265]\n",
      " [ 0.6926682   0.3073318 ]\n",
      " [ 0.79919939  0.20080061]\n",
      " [ 0.83011555  0.16988445]\n",
      " [ 0.5409675   0.4590325 ]\n",
      " [ 0.68302594  0.31697406]\n",
      " [ 0.79189528  0.20810472]\n",
      " [ 0.82368792  0.17631208]\n",
      " [ 0.52979663  0.47020337]\n",
      " [ 0.67322387  0.32677613]\n",
      " [ 0.78439716  0.21560284]\n",
      " [ 0.8170707   0.1829293 ]\n",
      " [ 0.51859583  0.48140417]\n",
      " [ 0.66326812  0.33673188]\n",
      " [ 0.77670506  0.22329494]\n",
      " [ 0.81026232  0.18973768]\n",
      " [ 0.5073763   0.4926237 ]\n",
      " [ 0.65316532  0.34683468]\n",
      " [ 0.76881941  0.23118059]\n",
      " [ 0.80326156  0.19673844]\n",
      " [ 0.49614934  0.50385066]\n",
      " [ 0.64292259  0.35707741]\n",
      " [ 0.76074106  0.23925894]\n",
      " [ 0.79606751  0.20393249]\n",
      " [ 0.48492626  0.51507374]\n",
      " [ 0.63254754  0.36745246]\n",
      " [ 0.7524713   0.2475287 ]\n",
      " [ 0.7886796   0.2113204 ]\n",
      " [ 0.47371837  0.52628163]\n",
      " [ 0.62204826  0.37795174]\n",
      " [ 0.7440119   0.2559881 ]\n",
      " [ 0.78109765  0.21890235]\n",
      " [ 0.46253689  0.53746311]\n",
      " [ 0.61143326  0.38856674]\n",
      " [ 0.73536506  0.26463494]\n",
      " [ 0.77332185  0.22667815]\n",
      " [ 0.45139297  0.54860703]\n",
      " [ 0.6007115   0.3992885 ]\n",
      " [ 0.7265335   0.2734665 ]\n",
      " [ 0.76535281  0.23464719]\n",
      " [ 0.52351189  0.47648811]\n",
      " [ 0.66765359  0.33234641]\n",
      " [ 0.78010297  0.21989703]\n",
      " [ 0.81327235  0.18672765]\n",
      " [ 0.51229919  0.48770081]\n",
      " [ 0.65761445  0.34238555]\n",
      " [ 0.77230214  0.22769786]\n",
      " [ 0.80635611  0.19364389]\n",
      " [ 0.5010741   0.4989259 ]\n",
      " [ 0.64743219  0.35256781]\n",
      " [ 0.76430818  0.23569182]\n",
      " [ 0.79924694  0.20075306]\n",
      " [ 0.48984793  0.51015207]\n",
      " [ 0.63711421  0.36288579]\n",
      " [ 0.75612219  0.24387781]\n",
      " [ 0.79194411  0.20805589]\n",
      " [ 0.47863198  0.52136802]\n",
      " [ 0.6266684   0.3733316 ]\n",
      " [ 0.74774573  0.25225427]\n",
      " [ 0.78444727  0.21555273]\n",
      " [ 0.46743754  0.53256246]\n",
      " [ 0.61610308  0.38389692]\n",
      " [ 0.7391808   0.2608192 ]\n",
      " [ 0.77675645  0.22324355]\n",
      " [ 0.45627579  0.54372421]\n",
      " [ 0.60542703  0.39457297]\n",
      " [ 0.73042991  0.26957009]\n",
      " [ 0.76887207  0.23112793]\n",
      " [ 0.44515778  0.55484222]\n",
      " [ 0.59464941  0.40535059]\n",
      " [ 0.72149603  0.27850397]\n",
      " [ 0.76079498  0.23920502]\n",
      " [ 0.43409439  0.56590561]\n",
      " [ 0.58377979  0.41622021]\n",
      " [ 0.71238265  0.28761735]\n",
      " [ 0.75252649  0.24747351]\n",
      " [ 0.42309629  0.57690371]\n",
      " [ 0.57282808  0.42717192]\n",
      " [ 0.70309379  0.29690621]\n",
      " [ 0.74406833  0.25593167]\n",
      " [ 0.49477156  0.50522844]\n",
      " [ 0.64165628  0.35834372]\n",
      " [ 0.75973643  0.24026357]\n",
      " [ 0.79517128  0.20482872]\n",
      " [ 0.48354974  0.51645026]\n",
      " [ 0.63126555  0.36873445]\n",
      " [ 0.7514433   0.2485567 ]\n",
      " [ 0.78775956  0.21224044]\n",
      " [ 0.47234448  0.52765552]\n",
      " [ 0.62075159  0.37924841]\n",
      " [ 0.74296076  0.25703924]\n",
      " [ 0.7801538   0.2198462 ]\n",
      " [ 0.46116702  0.53883298]\n",
      " [ 0.610123    0.389877  ]\n",
      " [ 0.7342911   0.2657089 ]\n",
      " [ 0.77235424  0.22764576]\n",
      " [ 0.45002847  0.54997153]\n",
      " [ 0.59938878  0.40061122]\n",
      " [ 0.72543708  0.27456292]\n",
      " [ 0.76436155  0.23563845]\n",
      " [ 0.43893979  0.56106021]\n",
      " [ 0.58855832  0.41144168]\n",
      " [ 0.71640198  0.28359802]\n",
      " [ 0.75617683  0.24382317]\n",
      " [ 0.42791176  0.57208824]\n",
      " [ 0.57764137  0.42235863]\n",
      " [ 0.70718956  0.29281044]\n",
      " [ 0.74780161  0.25219839]\n",
      " [ 0.41695489  0.58304511]\n",
      " [ 0.56664804  0.43335196]\n",
      " [ 0.69780413  0.30219587]\n",
      " [ 0.73923792  0.26076208]\n",
      " [ 0.40607943  0.59392057]\n",
      " [ 0.55558872  0.44441128]\n",
      " [ 0.68825047  0.31174953]\n",
      " [ 0.73048824  0.26951176]\n",
      " [ 0.39529531  0.60470469]\n",
      " [ 0.54447404  0.45552596]\n",
      " [ 0.67853393  0.32146607]\n",
      " [ 0.72155556  0.27844444]\n",
      " [ 0.46606574  0.53393426]\n",
      " [ 0.61479865  0.38520135]\n",
      " [ 0.73811681  0.26188319]\n",
      " [ 0.77579925  0.22420075]\n",
      " [ 0.45490876  0.54509124]\n",
      " [ 0.60410963  0.39589037]\n",
      " [ 0.72934329  0.27065671]\n",
      " [ 0.76789116  0.23210884]\n",
      " [ 0.44379687  0.55620313]\n",
      " [ 0.5933202   0.4066798 ]\n",
      " [ 0.72038718  0.27961282]\n",
      " [ 0.75979051  0.24020949]\n",
      " [ 0.43274093  0.56725907]\n",
      " [ 0.58243996  0.41756004]\n",
      " [ 0.71125204  0.28874796]\n",
      " [ 0.75149863  0.24850137]\n",
      " [ 0.42175156  0.57824844]\n",
      " [ 0.57147888  0.42852112]\n",
      " [ 0.70194194  0.29805806]\n",
      " [ 0.74301734  0.25698266]\n",
      " [ 0.41083915  0.58916085]\n",
      " [ 0.5604472   0.4395528 ]\n",
      " [ 0.69246145  0.30753855]\n",
      " [ 0.7343489   0.2656511 ]\n",
      " [ 0.40001377  0.59998623]\n",
      " [ 0.54935548  0.45064452]\n",
      " [ 0.68281566  0.31718434]\n",
      " [ 0.72549609  0.27450391]\n",
      " [ 0.38928515  0.61071485]\n",
      " [ 0.53821449  0.46178551]\n",
      " [ 0.67301021  0.32698979]\n",
      " [ 0.71646217  0.28353783]\n",
      " [ 0.37866268  0.62133732]\n",
      " [ 0.52703519  0.47296481]\n",
      " [ 0.66305121  0.33694879]\n",
      " [ 0.70725091  0.29274909]\n",
      " [ 0.36815534  0.63184466]\n",
      " [ 0.51582873  0.48417127]\n",
      " [ 0.6529453   0.3470547 ]\n",
      " [ 0.6978666   0.3021334 ]]\n"
     ]
    }
   ],
   "source": [
    "#this prints out two columns, one with probabilty of 1 and one with the probability of 0\n",
    "a = resultFit.predict_proba(train_cols_new)\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "      <th>intercept</th>\n",
       "      <th>prestige_2.0</th>\n",
       "      <th>prestige_3.0</th>\n",
       "      <th>prestige_4.0</th>\n",
       "      <th>predict_admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>220.0</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.289187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>220.0</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.182006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220.0</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>220.0</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.093077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>220.0</td>\n",
       "      <td>2.453333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.298505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     gre       gpa  prestige  intercept  prestige_2.0  prestige_3.0  \\\n",
       "0  220.0  2.260000       1.0        1.0             0             0   \n",
       "1  220.0  2.260000       2.0        1.0             1             0   \n",
       "2  220.0  2.260000       3.0        1.0             0             1   \n",
       "3  220.0  2.260000       4.0        1.0             0             0   \n",
       "4  220.0  2.453333       1.0        1.0             0             0   \n",
       "\n",
       "   prestige_4.0  predict_admit  \n",
       "0             0       0.289187  \n",
       "1             0       0.182006  \n",
       "2             0       0.111899  \n",
       "3             1       0.093077  \n",
       "4             0       0.298505  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combos['predict_admit'] = a[:,1:]\n",
    "combos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Interpret findings for the last 4 observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Here we predicted the probability of admittance with a new dataset and added those preditions to the dataframe.  We cans see thatg iven the same gpa(2.26) and gre(220), a student who attended a rank 4 school had a 9% probability of admittance while a student who attended a rank 1 school had a 30% probability of admittance.  Therefore we can say that students who attended a school of a higher prestige have a higher probability of addmittance into the graduate school program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Plot the probability of being admitted into graduate school, stratified by GPA and GRE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
